{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Advanced LangChain Document Loaders and Text Splitters\n",
    "\n",
    "This notebook demonstrates 5 additional loader/splitter classes not covered in the basic examples.\n",
    "\n",
    "## Overview\n",
    "We'll explore:\n",
    "1. **CSVLoader** - For structured data files\n",
    "2. **JSONLoader** - For JSON data with JQ queries  \n",
    "3. **DirectoryLoader** - For loading multiple files\n",
    "4. **RecursiveCharacterTextSplitter** - Intelligent text chunking\n",
    "5. **MarkdownHeaderTextSplitter** - Structure-aware splitting\n",
    "6. **TokenTextSplitter** - Token-based splitting for LLMs\n",
    "7. **Combined workflows** for real-world data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    DirectoryLoader,\n",
    "    TextLoader\n",
    ")\n",
    "\n",
    "# Text Splitters\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv_loader_section",
   "metadata": {},
   "source": [
    "## 1. CSV Loader - Loading Structured Data\n",
    "\n",
    "The CSVLoader is perfect for ingesting structured tabular data. It can handle various CSV formats and provides flexible configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "csv_loader_demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file created\n",
      "Content preview:\n",
      "name,age,department,salary\n",
      "John Doe,30,Engineering,75000\n",
      "Jane Smith,28,Marketing,65000\n",
      "Bob Johnson,35,Sales,70000\n",
      "Alice Brown,32,HR,60000\n",
      "Charlie Davis,29,Engineering,72000...\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV file for demonstration\n",
    "sample_csv_data = \"name,age,department,salary\\nJohn Doe,30,Engineering,75000\\nJane Smith,28,Marketing,65000\\nBob Johnson,35,Sales,70000\\nAlice Brown,32,HR,60000\\nCharlie Davis,29,Engineering,72000\"\n",
    "\n",
    "with open('employees.csv', 'w') as f:\n",
    "    f.write(sample_csv_data)\n",
    "\n",
    "print(\"Sample CSV file created\")\n",
    "print(\"Content preview:\")\n",
    "print(sample_csv_data[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "csv_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents from CSV\n",
      "Sample document metadata: {'source': 'employees.csv', 'row': 0}\n",
      "Sample content: name: name\n",
      "age: age\n",
      "department: department\n",
      "salary: salary\n",
      "Document type: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# Load CSV with different configurations\n",
    "csv_loader = CSVLoader(\n",
    "    file_path='employees.csv',\n",
    "    csv_args={\n",
    "        'delimiter': ',',\n",
    "        'quotechar': '\"',\n",
    "        'fieldnames': ['name', 'age', 'department', 'salary']\n",
    "    }\n",
    ")\n",
    "\n",
    "csv_documents = csv_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(csv_documents)} documents from CSV\")\n",
    "print(f\"Sample document metadata: {csv_documents[0].metadata}\")\n",
    "print(f\"Sample content: {csv_documents[0].page_content}\")\n",
    "print(f\"Document type: {type(csv_documents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json_loader_section",
   "metadata": {},
   "source": [
    "## 2. JSON Loader - Loading JSON Data with JQ Queries\n",
    "\n",
    "The JSONLoader can extract specific parts of JSON documents using JQ queries, making it powerful for complex nested data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "json_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample JSON file created\n",
      "Number of articles: 2\n"
     ]
    }
   ],
   "source": [
    "# Create sample JSON data\n",
    "sample_json_data = {\n",
    "    \"articles\": [\n",
    "        {\n",
    "            \"title\": \"Introduction to Machine Learning\",\n",
    "            \"author\": \"Dr. Smith\",\n",
    "            \"content\": \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "            \"tags\": [\"AI\", \"ML\", \"Technology\"],\n",
    "            \"published_date\": \"2024-01-15\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Deep Learning Fundamentals\",\n",
    "            \"author\": \"Prof. Johnson\",\n",
    "            \"content\": \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "            \"tags\": [\"Deep Learning\", \"Neural Networks\", \"AI\"],\n",
    "            \"published_date\": \"2024-01-20\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('articles.json', 'w') as f:\n",
    "    json.dump(sample_json_data, f, indent=2)\n",
    "\n",
    "print(\"Sample JSON file created\")\n",
    "print(f\"Number of articles: {len(sample_json_data['articles'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "json_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents from JSON\n",
      "First document:\n",
      "Metadata: {'source': '/Users/ortimus/dev/KrishnaikAcademy/AgenticAI/articles.json', 'seq_num': 1, 'title': 'Introduction to Machine Learning', 'author': 'Dr. Smith', 'tags': ['AI', 'ML', 'Technology'], 'published_date': '2024-01-15'}\n",
      "Content: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n"
     ]
    }
   ],
   "source": [
    "# Load JSON with JQ query to extract specific fields\n",
    "json_loader = JSONLoader(\n",
    "    file_path='articles.json',\n",
    "    jq_schema='.articles[]',  # Extract each article\n",
    "    content_key='content',    # Use 'content' field as page_content\n",
    "    metadata_func=lambda record, metadata: {\n",
    "        **metadata,\n",
    "        \"title\": record.get(\"title\"),\n",
    "        \"author\": record.get(\"author\"),\n",
    "        \"tags\": record.get(\"tags\"),\n",
    "        \"published_date\": record.get(\"published_date\")\n",
    "    }\n",
    ")\n",
    "\n",
    "json_documents = json_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(json_documents)} documents from JSON\")\n",
    "print(\"First document:\")\n",
    "print(f\"Metadata: {json_documents[0].metadata}\")\n",
    "print(f\"Content: {json_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directory_loader_section",
   "metadata": {},
   "source": [
    "## 3. Directory Loader - Loading Multiple Files\n",
    "\n",
    "The DirectoryLoader enables bulk loading of multiple files from a directory, with support for file pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "directory_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory with 3 files\n"
     ]
    }
   ],
   "source": [
    "# Create a sample directory structure\n",
    "os.makedirs('sample_docs', exist_ok=True)\n",
    "\n",
    "# Create sample text files\n",
    "sample_texts = {\n",
    "    'nlp_basics.txt': \"Natural Language Processing (NLP) is a field of AI that focuses on human language.\",\n",
    "    'computer_vision.txt': \"Computer Vision trains computers to interpret and understand visual information.\",\n",
    "    'robotics_ai.txt': \"Robotics combines AI, engineering, and physics to create intelligent machines.\"\n",
    "}\n",
    "\n",
    "for filename, content in sample_texts.items():\n",
    "    with open(f'sample_docs/{filename}', 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"Created directory with {len(sample_texts)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "directory_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents from directory\n",
      "  - nlp_basics.txt: 82 characters\n",
      "  - computer_vision.txt: 80 characters\n",
      "  - robotics_ai.txt: 78 characters\n"
     ]
    }
   ],
   "source": [
    "# Load all text files from directory\n",
    "directory_loader = DirectoryLoader(\n",
    "    'sample_docs/',\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "directory_documents = directory_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(directory_documents)} documents from directory\")\n",
    "for doc in directory_documents:\n",
    "    filename = os.path.basename(doc.metadata['source'])\n",
    "    print(f\"  - {filename}: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recursive_splitter_section",
   "metadata": {},
   "source": [
    "## 4. Recursive Character Text Splitter - Intelligent Text Chunking\n",
    "\n",
    "The RecursiveCharacterTextSplitter tries to split text at natural boundaries, respecting document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "long_text_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text prepared (800 characters)\n"
     ]
    }
   ],
   "source": [
    "# Sample long text for splitting\n",
    "long_text = \"\"\"# Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is a rapidly evolving field that aims to create machines capable of intelligent behavior.\n",
    "\n",
    "## History of AI\n",
    "\n",
    "The field of AI was founded in 1956 at a conference at Dartmouth College. Early pioneers included Alan Turing, John McCarthy, and Marvin Minsky.\n",
    "\n",
    "### Early Developments\n",
    "- 1950: Alan Turing publishes Computing Machinery and Intelligence\n",
    "- 1956: The term artificial intelligence is coined\n",
    "- 1960s: First AI programs developed\n",
    "\n",
    "## Applications of AI\n",
    "\n",
    "AI has numerous applications across various industries:\n",
    "\n",
    "1. Healthcare: Diagnostic imaging, drug discovery\n",
    "2. Transportation: Autonomous vehicles, traffic optimization\n",
    "3. Finance: Fraud detection, algorithmic trading\n",
    "4. Education: Personalized learning, automated grading\"\"\"\n",
    "\n",
    "print(f\"Long text prepared ({len(long_text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recursive_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 4 chunks\n",
      "Average chunk size: 203.0 characters\n",
      "\n",
      "Chunk 1:\n",
      "# Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is a rapidly evolving field that aims to create machines capable of intelligen...\n",
      "\n",
      "Chunk 2:\n",
      "## History of AI\n",
      "\n",
      "The field of AI was founded in 1956 at a conference at Dartmouth College. Early pioneers included Alan Turing, John McCarthy, and Ma...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the splitter with various parameters\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = recursive_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split text into {len(chunks)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in chunks) / len(chunks):.1f} characters\")\n",
    "\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk[:150] + \"...\" if len(chunk) > 150 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_splitter_section",
   "metadata": {},
   "source": [
    "## 5. Markdown Header Text Splitter - Structure-Aware Splitting\n",
    "\n",
    "The MarkdownHeaderTextSplitter splits text based on markdown headers, preserving document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "markdown_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split markdown into 4 sections\n",
      "\n",
      "Chunk 1:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence'}\n",
      "Content preview: # Introduction to Artificial Intelligence  \n",
      "Artificial Intelligence (AI) is a rapidly evolving field...\n",
      "\n",
      "Chunk 2:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence', 'Header 2': 'History of AI'}\n",
      "Content preview: ## History of AI  \n",
      "The field of AI was founded in 1956 at a conference at Dartmouth College. Early p...\n",
      "\n",
      "Chunk 3:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence', 'Header 2': 'History of AI', 'Header 3': 'Early Developments'}\n",
      "Content preview: ### Early Developments\n",
      "- 1950: Alan Turing publishes Computing Machinery and Intelligence\n",
      "- 1956: Th...\n"
     ]
    }
   ],
   "source": [
    "# Define headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# Initialize the markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "# Split the markdown text\n",
    "markdown_chunks = markdown_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split markdown into {len(markdown_chunks)} sections\")\n",
    "\n",
    "for i, chunk in enumerate(markdown_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token_splitter_section",
   "metadata": {},
   "source": [
    "## 6. Token Text Splitter - Token-Based Chunking for LLMs\n",
    "\n",
    "The TokenTextSplitter splits text based on tokens rather than characters, crucial for LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "token_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 2 token-based chunks\n",
      "Token counts: [100, 79]\n",
      "Average tokens per chunk: 89.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize token splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "# Split text by tokens\n",
    "token_chunks = token_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split text into {len(token_chunks)} token-based chunks\")\n",
    "\n",
    "# Analyze token counts\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_counts = [len(encoding.encode(chunk)) for chunk in token_chunks]\n",
    "print(f\"Token counts: {token_counts}\")\n",
    "print(f\"Average tokens per chunk: {sum(token_counts) / len(token_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined_workflow_section",
   "metadata": {},
   "source": [
    "## 7. Combining Loaders with Splitters - Real-World Workflow\n",
    "\n",
    "In practice, you'll combine document loaders with text splitters for complete data processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "workflow_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow functions defined\n"
     ]
    }
   ],
   "source": [
    "def process_documents_workflow(documents: List[Document], splitter) -> List[Document]:\n",
    "    \"\"\"Process documents through a splitting workflow\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = splitter.split_text(doc.page_content)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            )\n",
    "            all_chunks.append(chunk_doc)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"Workflow functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 6 docs → 6 chunks\n",
      "JSON: 2 docs → 2 chunks\n",
      "Directory: 3 docs → 3 chunks\n",
      "\n",
      "Total: 11 docs → 11 chunks\n"
     ]
    }
   ],
   "source": [
    "# Process different document types with different splitters\n",
    "\n",
    "# Process CSV documents with recursive splitter\n",
    "processed_csv = process_documents_workflow(csv_documents, recursive_splitter)\n",
    "print(f\"CSV: {len(csv_documents)} docs → {len(processed_csv)} chunks\")\n",
    "\n",
    "# Process JSON documents with token splitter\n",
    "processed_json = process_documents_workflow(json_documents, token_splitter)\n",
    "print(f\"JSON: {len(json_documents)} docs → {len(processed_json)} chunks\")\n",
    "\n",
    "# Process directory documents\n",
    "processed_directory = process_documents_workflow(directory_documents, recursive_splitter)\n",
    "print(f\"Directory: {len(directory_documents)} docs → {len(processed_directory)} chunks\")\n",
    "\n",
    "total_original = len(csv_documents + json_documents + directory_documents)\n",
    "total_processed = len(processed_csv + processed_json + processed_directory)\n",
    "print(f\"\\nTotal: {total_original} docs → {total_processed} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_section",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning up temporary files...\n",
      "Removed employees.csv\n",
      "\n",
      "Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "def cleanup_files():\n",
    "    \"\"\"Remove temporary files created during the demo\"\"\"\n",
    "    files_to_remove = ['employees.csv', 'articles.json']\n",
    "    dirs_to_remove = ['sample_docs']\n",
    "    \n",
    "    print(\"🧹 Cleaning up temporary files...\")\n",
    "    \n",
    "    for file in files_to_remove:\n",
    "        try:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "                print(f\"Removed {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {file}: {e}\")\n",
    "    \n",
    "    for dir in dirs_to_remove:\n",
    "        try:\n",
    "            if os.path.exists(dir):\n",
    "                shutil.rmtree(dir)\n",
    "                print(f\"Removed directory {dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {dir}: {e}\")\n",
    "    \n",
    "    print(\"\\nCleanup completed!\")\n",
    "\n",
    "cleanup_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE SUMMARY\n",
      "==================================================\n",
      "\n",
      "DOCUMENT LOADERS COVERED:\n",
      "1. CSVLoader - Structured tabular data with flexible parsing\n",
      "2. JSONLoader - Complex JSON with JQ query support\n",
      "3. DirectoryLoader - Bulk loading from file systems\n",
      "\n",
      "TEXT SPLITTERS COVERED:\n",
      "4. RecursiveCharacterTextSplitter - Intelligent hierarchy-based splitting\n",
      "5. MarkdownHeaderTextSplitter - Structure-aware markdown processing\n",
      "6. TokenTextSplitter - Token-based splitting for LLM applications\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "• Choose loaders based on data format and complexity\n",
      "• Select splitters based on your application's needs\n",
      "• Consider token limits for LLM applications\n",
      "• Preserve important metadata throughout processing\n",
      "• Test different configurations for optimal results\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nDOCUMENT LOADERS COVERED:\")\n",
    "print(\"1. CSVLoader - Structured tabular data with flexible parsing\")\n",
    "print(\"2. JSONLoader - Complex JSON with JQ query support\")\n",
    "print(\"3. DirectoryLoader - Bulk loading from file systems\")\n",
    "\n",
    "print(\"\\nTEXT SPLITTERS COVERED:\")\n",
    "print(\"4. RecursiveCharacterTextSplitter - Intelligent hierarchy-based splitting\")\n",
    "print(\"5. MarkdownHeaderTextSplitter - Structure-aware markdown processing\")\n",
    "print(\"6. TokenTextSplitter - Token-based splitting for LLM applications\")\n",
    "\n",
    "print(\"\\nKEY TAKEAWAYS:\")\n",
    "print(\"• Choose loaders based on data format and complexity\")\n",
    "print(\"• Select splitters based on your application's needs\")\n",
    "print(\"• Consider token limits for LLM applications\")\n",
    "print(\"• Preserve important metadata throughout processing\")\n",
    "print(\"• Test different configurations for optimal results\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
