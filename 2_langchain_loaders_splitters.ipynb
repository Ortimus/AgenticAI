{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Advanced LangChain Document Loaders and Text Splitters\n",
    "\n",
    "This notebook demonstrates 5 additional loader/splitter classes not covered in the basic examples.\n",
    "\n",
    "## Overview\n",
    "We'll explore:\n",
    "1. **CSVLoader** - For structured data files\n",
    "2. **JSONLoader** - For JSON data with JQ queries  \n",
    "3. **DirectoryLoader** - For loading multiple files\n",
    "4. **RecursiveCharacterTextSplitter** - Intelligent text chunking\n",
    "5. **MarkdownHeaderTextSplitter** - Structure-aware splitting\n",
    "6. **TokenTextSplitter** - Token-based splitting for LLMs\n",
    "7. **Combined workflows** for real-world data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "# Core LangChain imports\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Document Loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    DirectoryLoader,\n",
    "    TextLoader\n",
    ")\n",
    "\n",
    "# Text Splitters\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv_loader_section",
   "metadata": {},
   "source": [
    "## 1. CSV Loader - Loading Structured Data\n",
    "\n",
    "The CSVLoader is perfect for ingesting structured tabular data. It can handle various CSV formats and provides flexible configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "csv_loader_demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV file created\n",
      "Content preview:\n",
      "name,age,department,salary\n",
      "John Doe,30,Engineering,75000\n",
      "Jane Smith,28,Marketing,65000\n",
      "Bob Johnson,35,Sales,70000\n",
      "Alice Brown,32,HR,60000\n",
      "Charlie Davis,29,Engineering,72000...\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV file for demonstration\n",
    "sample_csv_data = \"name,age,department,salary\\nJohn Doe,30,Engineering,75000\\nJane Smith,28,Marketing,65000\\nBob Johnson,35,Sales,70000\\nAlice Brown,32,HR,60000\\nCharlie Davis,29,Engineering,72000\"\n",
    "\n",
    "with open('employees.csv', 'w') as f:\n",
    "    f.write(sample_csv_data)\n",
    "\n",
    "print(\"Sample CSV file created\")\n",
    "print(\"Content preview:\")\n",
    "print(sample_csv_data[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "csv_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents from CSV\n",
      "Sample document metadata: {'source': 'employees.csv', 'row': 0}\n",
      "Sample content: name: name\n",
      "age: age\n",
      "department: department\n",
      "salary: salary\n",
      "Document type: <class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# Load CSV with different configurations\n",
    "csv_loader = CSVLoader(\n",
    "    file_path='employees.csv',\n",
    "    csv_args={\n",
    "        'delimiter': ',',\n",
    "        'quotechar': '\"',\n",
    "        'fieldnames': ['name', 'age', 'department', 'salary']\n",
    "    }\n",
    ")\n",
    "\n",
    "csv_documents = csv_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(csv_documents)} documents from CSV\")\n",
    "print(f\"Sample document metadata: {csv_documents[0].metadata}\")\n",
    "print(f\"Sample content: {csv_documents[0].page_content}\")\n",
    "print(f\"Document type: {type(csv_documents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json_loader_section",
   "metadata": {},
   "source": [
    "## 2. JSON Loader - Loading JSON Data with JQ Queries\n",
    "\n",
    "The JSONLoader can extract specific parts of JSON documents using JQ queries, making it powerful for complex nested data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "json_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample JSON file created\n",
      "Number of articles: 2\n"
     ]
    }
   ],
   "source": [
    "# Create sample JSON data\n",
    "sample_json_data = {\n",
    "    \"articles\": [\n",
    "        {\n",
    "            \"title\": \"Introduction to Machine Learning\",\n",
    "            \"author\": \"Dr. Smith\",\n",
    "            \"content\": \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "            \"tags\": [\"AI\", \"ML\", \"Technology\"],\n",
    "            \"published_date\": \"2024-01-15\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Deep Learning Fundamentals\",\n",
    "            \"author\": \"Prof. Johnson\",\n",
    "            \"content\": \"Deep learning uses neural networks with multiple layers to model and understand complex patterns in data.\",\n",
    "            \"tags\": [\"Deep Learning\", \"Neural Networks\", \"AI\"],\n",
    "            \"published_date\": \"2024-01-20\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('articles.json', 'w') as f:\n",
    "    json.dump(sample_json_data, f, indent=2)\n",
    "\n",
    "print(\"Sample JSON file created\")\n",
    "print(f\"Number of articles: {len(sample_json_data['articles'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "json_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 documents from JSON\n",
      "First document:\n",
      "Metadata: {'source': '/Users/ortimus/dev/KrishnaikAcademy/AgenticAI/articles.json', 'seq_num': 1, 'title': 'Introduction to Machine Learning', 'author': 'Dr. Smith', 'tags': ['AI', 'ML', 'Technology'], 'published_date': '2024-01-15'}\n",
      "Content: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n"
     ]
    }
   ],
   "source": [
    "# Load JSON with JQ query to extract specific fields\n",
    "json_loader = JSONLoader(\n",
    "    file_path='articles.json',\n",
    "    jq_schema='.articles[]',  # Extract each article\n",
    "    content_key='content',    # Use 'content' field as page_content\n",
    "    metadata_func=lambda record, metadata: {\n",
    "        **metadata,\n",
    "        \"title\": record.get(\"title\"),\n",
    "        \"author\": record.get(\"author\"),\n",
    "        \"tags\": record.get(\"tags\"),\n",
    "        \"published_date\": record.get(\"published_date\")\n",
    "    }\n",
    ")\n",
    "\n",
    "json_documents = json_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(json_documents)} documents from JSON\")\n",
    "print(\"First document:\")\n",
    "print(f\"Metadata: {json_documents[0].metadata}\")\n",
    "print(f\"Content: {json_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directory_loader_section",
   "metadata": {},
   "source": [
    "## 3. Directory Loader - Loading Multiple Files\n",
    "\n",
    "The DirectoryLoader enables bulk loading of multiple files from a directory, with support for file pattern matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "directory_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory with 3 files\n"
     ]
    }
   ],
   "source": [
    "# Create a sample directory structure\n",
    "os.makedirs('sample_docs', exist_ok=True)\n",
    "\n",
    "# Create sample text files\n",
    "sample_texts = {\n",
    "    'nlp_basics.txt': \"Natural Language Processing (NLP) is a field of AI that focuses on human language.\",\n",
    "    'computer_vision.txt': \"Computer Vision trains computers to interpret and understand visual information.\",\n",
    "    'robotics_ai.txt': \"Robotics combines AI, engineering, and physics to create intelligent machines.\"\n",
    "}\n",
    "\n",
    "for filename, content in sample_texts.items():\n",
    "    with open(f'sample_docs/{filename}', 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"Created directory with {len(sample_texts)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "directory_loader_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents from directory\n",
      "  - nlp_basics.txt: 82 characters\n",
      "  - computer_vision.txt: 80 characters\n",
      "  - robotics_ai.txt: 78 characters\n"
     ]
    }
   ],
   "source": [
    "# Load all text files from directory\n",
    "directory_loader = DirectoryLoader(\n",
    "    'sample_docs/',\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "directory_documents = directory_loader.load()\n",
    "\n",
    "print(f\"Loaded {len(directory_documents)} documents from directory\")\n",
    "for doc in directory_documents:\n",
    "    filename = os.path.basename(doc.metadata['source'])\n",
    "    print(f\"  - {filename}: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recursive_splitter_section",
   "metadata": {},
   "source": [
    "## 4. Recursive Character Text Splitter - Intelligent Text Chunking\n",
    "\n",
    "The RecursiveCharacterTextSplitter tries to split text at natural boundaries, respecting document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "long_text_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long text prepared (800 characters)\n"
     ]
    }
   ],
   "source": [
    "# Sample long text for splitting\n",
    "long_text = \"\"\"# Introduction to Artificial Intelligence\n",
    "\n",
    "Artificial Intelligence (AI) is a rapidly evolving field that aims to create machines capable of intelligent behavior.\n",
    "\n",
    "## History of AI\n",
    "\n",
    "The field of AI was founded in 1956 at a conference at Dartmouth College. Early pioneers included Alan Turing, John McCarthy, and Marvin Minsky.\n",
    "\n",
    "### Early Developments\n",
    "- 1950: Alan Turing publishes Computing Machinery and Intelligence\n",
    "- 1956: The term artificial intelligence is coined\n",
    "- 1960s: First AI programs developed\n",
    "\n",
    "## Applications of AI\n",
    "\n",
    "AI has numerous applications across various industries:\n",
    "\n",
    "1. Healthcare: Diagnostic imaging, drug discovery\n",
    "2. Transportation: Autonomous vehicles, traffic optimization\n",
    "3. Finance: Fraud detection, algorithmic trading\n",
    "4. Education: Personalized learning, automated grading\"\"\"\n",
    "\n",
    "print(f\"Long text prepared ({len(long_text)} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "recursive_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 4 chunks\n",
      "Average chunk size: 203.0 characters\n",
      "\n",
      "Chunk 1:\n",
      "# Introduction to Artificial Intelligence\n",
      "\n",
      "Artificial Intelligence (AI) is a rapidly evolving field that aims to create machines capable of intelligen...\n",
      "\n",
      "Chunk 2:\n",
      "## History of AI\n",
      "\n",
      "The field of AI was founded in 1956 at a conference at Dartmouth College. Early pioneers included Alan Turing, John McCarthy, and Ma...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the splitter with various parameters\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# Split the text\n",
    "chunks = recursive_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split text into {len(chunks)} chunks\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in chunks) / len(chunks):.1f} characters\")\n",
    "\n",
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(chunk[:150] + \"...\" if len(chunk) > 150 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "markdown_splitter_section",
   "metadata": {},
   "source": [
    "## 5. Markdown Header Text Splitter - Structure-Aware Splitting\n",
    "\n",
    "The MarkdownHeaderTextSplitter splits text based on markdown headers, preserving document structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "markdown_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split markdown into 4 sections\n",
      "\n",
      "Chunk 1:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence'}\n",
      "Content preview: # Introduction to Artificial Intelligence  \n",
      "Artificial Intelligence (AI) is a rapidly evolving field...\n",
      "\n",
      "Chunk 2:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence', 'Header 2': 'History of AI'}\n",
      "Content preview: ## History of AI  \n",
      "The field of AI was founded in 1956 at a conference at Dartmouth College. Early p...\n",
      "\n",
      "Chunk 3:\n",
      "Metadata: {'Header 1': 'Introduction to Artificial Intelligence', 'Header 2': 'History of AI', 'Header 3': 'Early Developments'}\n",
      "Content preview: ### Early Developments\n",
      "- 1950: Alan Turing publishes Computing Machinery and Intelligence\n",
      "- 1956: Th...\n"
     ]
    }
   ],
   "source": [
    "# Define headers to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "# Initialize the markdown splitter\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    ")\n",
    "\n",
    "# Split the markdown text\n",
    "markdown_chunks = markdown_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split markdown into {len(markdown_chunks)} sections\")\n",
    "\n",
    "for i, chunk in enumerate(markdown_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Content preview: {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "token_splitter_section",
   "metadata": {},
   "source": [
    "## 6. Token Text Splitter - Token-Based Chunking for LLMs\n",
    "\n",
    "The TokenTextSplitter splits text based on tokens rather than characters, crucial for LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "token_splitter_usage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split text into 2 token-based chunks\n",
      "Token counts: [100, 79]\n",
      "Average tokens per chunk: 89.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize token splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    encoding_name=\"cl100k_base\"\n",
    ")\n",
    "\n",
    "# Split text by tokens\n",
    "token_chunks = token_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Split text into {len(token_chunks)} token-based chunks\")\n",
    "\n",
    "# Analyze token counts\n",
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_counts = [len(encoding.encode(chunk)) for chunk in token_chunks]\n",
    "print(f\"Token counts: {token_counts}\")\n",
    "print(f\"Average tokens per chunk: {sum(token_counts) / len(token_counts):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined_workflow_section",
   "metadata": {},
   "source": [
    "## 7. Combining Loaders with Splitters - Real-World Workflow\n",
    "\n",
    "In practice, you'll combine document loaders with text splitters for complete data processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "workflow_functions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow functions defined\n"
     ]
    }
   ],
   "source": [
    "def process_documents_workflow(documents: List[Document], splitter) -> List[Document]:\n",
    "    \"\"\"Process documents through a splitting workflow\"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        chunks = splitter.split_text(doc.page_content)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    **doc.metadata,\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            )\n",
    "            all_chunks.append(chunk_doc)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "print(\"Workflow functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "workflow_execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV: 6 docs â†’ 6 chunks\n",
      "JSON: 2 docs â†’ 2 chunks\n",
      "Directory: 3 docs â†’ 3 chunks\n",
      "\n",
      "Total: 11 docs â†’ 11 chunks\n"
     ]
    }
   ],
   "source": [
    "# Process different document types with different splitters\n",
    "\n",
    "# Process CSV documents with recursive splitter\n",
    "processed_csv = process_documents_workflow(csv_documents, recursive_splitter)\n",
    "print(f\"CSV: {len(csv_documents)} docs â†’ {len(processed_csv)} chunks\")\n",
    "\n",
    "# Process JSON documents with token splitter\n",
    "processed_json = process_documents_workflow(json_documents, token_splitter)\n",
    "print(f\"JSON: {len(json_documents)} docs â†’ {len(processed_json)} chunks\")\n",
    "\n",
    "# Process directory documents\n",
    "processed_directory = process_documents_workflow(directory_documents, recursive_splitter)\n",
    "print(f\"Directory: {len(directory_documents)} docs â†’ {len(processed_directory)} chunks\")\n",
    "\n",
    "total_original = len(csv_documents + json_documents + directory_documents)\n",
    "total_processed = len(processed_csv + processed_json + processed_directory)\n",
    "print(f\"\\nTotal: {total_original} docs â†’ {total_processed} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup_section",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cleanup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning up temporary files...\n",
      "Removed employees.csv\n",
      "\n",
      "Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Clean up temporary files\n",
    "def cleanup_files():\n",
    "    \"\"\"Remove temporary files created during the demo\"\"\"\n",
    "    files_to_remove = ['employees.csv', 'articles.json']\n",
    "    dirs_to_remove = ['sample_docs']\n",
    "    \n",
    "    print(\"ðŸ§¹ Cleaning up temporary files...\")\n",
    "    \n",
    "    for file in files_to_remove:\n",
    "        try:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "                print(f\"Removed {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {file}: {e}\")\n",
    "    \n",
    "    for dir in dirs_to_remove:\n",
    "        try:\n",
    "            if os.path.exists(dir):\n",
    "                shutil.rmtree(dir)\n",
    "                print(f\"Removed directory {dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing {dir}: {e}\")\n",
    "    \n",
    "    print(\"\\nCleanup completed!\")\n",
    "\n",
    "cleanup_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "final_summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE SUMMARY\n",
      "==================================================\n",
      "\n",
      "DOCUMENT LOADERS COVERED:\n",
      "1. CSVLoader - Structured tabular data with flexible parsing\n",
      "2. JSONLoader - Complex JSON with JQ query support\n",
      "3. DirectoryLoader - Bulk loading from file systems\n",
      "\n",
      "TEXT SPLITTERS COVERED:\n",
      "4. RecursiveCharacterTextSplitter - Intelligent hierarchy-based splitting\n",
      "5. MarkdownHeaderTextSplitter - Structure-aware markdown processing\n",
      "6. TokenTextSplitter - Token-based splitting for LLM applications\n",
      "\n",
      "KEY TAKEAWAYS:\n",
      "â€¢ Choose loaders based on data format and complexity\n",
      "â€¢ Select splitters based on your application's needs\n",
      "â€¢ Consider token limits for LLM applications\n",
      "â€¢ Preserve important metadata throughout processing\n",
      "â€¢ Test different configurations for optimal results\n"
     ]
    }
   ],
   "source": [
    "print(\"COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nDOCUMENT LOADERS COVERED:\")\n",
    "print(\"1. CSVLoader - Structured tabular data with flexible parsing\")\n",
    "print(\"2. JSONLoader - Complex JSON with JQ query support\")\n",
    "print(\"3. DirectoryLoader - Bulk loading from file systems\")\n",
    "\n",
    "print(\"\\nTEXT SPLITTERS COVERED:\")\n",
    "print(\"4. RecursiveCharacterTextSplitter - Intelligent hierarchy-based splitting\")\n",
    "print(\"5. MarkdownHeaderTextSplitter - Structure-aware markdown processing\")\n",
    "print(\"6. TokenTextSplitter - Token-based splitting for LLM applications\")\n",
    "\n",
    "print(\"\\nKEY TAKEAWAYS:\")\n",
    "print(\"â€¢ Choose loaders based on data format and complexity\")\n",
    "print(\"â€¢ Select splitters based on your application's needs\")\n",
    "print(\"â€¢ Consider token limits for LLM applications\")\n",
    "print(\"â€¢ Preserve important metadata throughout processing\")\n",
    "print(\"â€¢ Test different configurations for optimal results\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
